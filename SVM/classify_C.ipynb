{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9\n",
    "\n",
    "**12210357 徐婧珺**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using Python and Numpy, write a class named SVMClassifier, which implements the SVM algorithm having slack variables and kernels such as Polynomial, Gaussian, and Sigmoid (using cvxopt package to solve the quadratic programing problem for Lagrange multipliers).\n",
    "* Consider the dataset of letter recognition (named letter-recognition.data). The dataset has 20,000 samples, for which the first column indicates the class (A~Z, totally 26 classes), and the rest columns indicate 16 features as described in the following table. For this dataset, use SVM to do a binary classification for letter ‘C’ or non-‘C’class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'svm_classifier' from '/mnt/e/桌面/专业学习/SIM207/Python/W15/svm_classifier.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import svm_classifier as svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import importlib\n",
    "\n",
    "importlib.reload(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Class SVMClassifier is written in the file 'svm_classifier.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据进行预处理\n",
    "def date_preprocess(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            # 假设每行数据以逗号分隔\n",
    "            data.append(line.strip().split(','))\n",
    "\n",
    "    # 转换为 NumPy 数组\n",
    "    data = np.array(data)\n",
    "\n",
    "    # 处理标签：'C' 转换为 1，其他转换为 -1\n",
    "    labels = np.where(data[:, 0] == 'C', 1, -1)\n",
    "\n",
    "    # 提取特征，并转换为浮点数\n",
    "    features = data[:, 1:].astype(float)\n",
    "\n",
    "    return labels, features\n",
    "\n",
    "labels, features = date_preprocess('letter-recognition.data')\n",
    "train_features, test_features, train_labels, test_labels=train_test_split(features, labels, test_size = 0.5) # 输出顺序train_input, test_input, train_target, test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the limited computer memory, the python kernel will crash when computng the solutions of alphas using cvxopt. Thus, only part of the data is used to implement the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian kernel\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.5312e+03 -1.4651e+05  4e+05  5e-01  2e-14\n",
      " 1:  1.8841e+03 -1.9467e+04  3e+04  2e-02  7e-15\n",
      " 2:  7.9990e+02 -5.8015e+03  8e+03  4e-03  1e-14\n",
      " 3:  1.4936e+02 -2.1506e+03  2e+03  9e-04  1e-14\n",
      " 4: -7.0759e+01 -1.0447e+03  1e+03  2e-13  7e-15\n",
      " 5: -1.5526e+02 -5.0942e+02  4e+02  2e-14  6e-15\n",
      " 6: -1.8114e+02 -3.3060e+02  1e+02  3e-14  4e-15\n",
      " 7: -1.9217e+02 -2.4417e+02  5e+01  2e-14  4e-15\n",
      " 8: -1.9618e+02 -2.1623e+02  2e+01  6e-14  4e-15\n",
      " 9: -1.9781e+02 -2.0633e+02  9e+00  2e-14  4e-15\n",
      "10: -1.9861e+02 -2.0162e+02  3e+00  2e-14  4e-15\n",
      "11: -1.9899e+02 -1.9982e+02  8e-01  6e-14  4e-15\n",
      "12: -1.9912e+02 -1.9936e+02  2e-01  2e-14  4e-15\n",
      "13: -1.9917e+02 -1.9923e+02  7e-02  6e-15  4e-15\n",
      "14: -1.9918e+02 -1.9919e+02  9e-03  5e-14  4e-15\n",
      "15: -1.9918e+02 -1.9919e+02  1e-03  4e-14  4e-15\n",
      "16: -1.9919e+02 -1.9919e+02  4e-05  2e-14  5e-15\n",
      "Optimal solution found.\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 仅使用50%的数据\n",
    "print(\"Gaussian kernel\")\n",
    "model = svm.SVMClassifier()\n",
    "model.fit(train_features, train_labels, 'Gaussian', 6.5)\n",
    "pred_labels = model.predict(train_features)\n",
    "print(accuracy_score(train_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial kernel\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5160e-03 -1.0105e+04  4e+04  1e+00  3e-11\n",
      " 1:  7.5553e-02 -2.7352e+03  3e+03  3e-02  4e-11\n",
      " 2:  1.4085e-02 -2.0934e+02  2e+02  2e-03  4e-11\n",
      " 3:  1.1287e-03 -4.7959e+01  6e+01  4e-04  2e-11\n",
      " 4:  1.1787e-02 -2.0315e+01  2e+01  2e-04  8e-12\n",
      " 5:  5.7821e-03 -5.5809e+00  7e+00  4e-05  4e-12\n",
      " 6:  4.3482e-03 -1.4179e+00  2e+00  8e-06  2e-12\n",
      " 7:  7.5666e-03 -6.9867e-01  9e-01  3e-06  9e-13\n",
      " 8:  4.4874e-03 -3.1745e-01  4e-01  1e-06  5e-13\n",
      " 9:  2.0697e-03 -1.6347e-01  2e-01  5e-07  2e-13\n",
      "10:  6.4160e-04 -9.7736e-02  1e-01  3e-07  1e-13\n",
      "11:  4.4691e-05 -6.4947e-02  8e-02  1e-07  9e-14\n",
      "12: -6.4448e-04 -4.7121e-02  5e-02  8e-08  8e-14\n",
      "13: -9.5688e-04 -3.5723e-02  4e-02  5e-08  1e-13\n",
      "14: -1.9674e-03 -2.4036e-02  2e-02  2e-08  1e-13\n",
      "15: -2.7775e-03 -1.5052e-02  1e-02  4e-09  1e-13\n",
      "16: -4.0494e-03 -9.3045e-03  5e-03  1e-09  6e-13\n",
      "17: -4.6305e-03 -6.9407e-03  2e-03  3e-10  9e-14\n",
      "18: -4.8917e-03 -6.0491e-03  1e-03  2e-16  7e-14\n",
      "19: -5.0806e-03 -5.5649e-03  5e-04  2e-16  9e-14\n",
      "20: -5.1021e-03 -5.5133e-03  4e-04  2e-16  7e-14\n",
      "21: -5.1776e-03 -5.3477e-03  2e-04  2e-16  9e-14\n",
      "22: -5.1976e-03 -5.3056e-03  1e-04  2e-16  8e-14\n",
      "23: -5.2209e-03 -5.2649e-03  4e-05  2e-16  7e-14\n",
      "24: -5.2228e-03 -5.2609e-03  4e-05  2e-16  8e-14\n",
      "25: -5.2340e-03 -5.2447e-03  1e-05  2e-16  8e-14\n",
      "26: -5.2377e-03 -5.2394e-03  2e-06  2e-16  9e-14\n",
      "27: -5.2385e-03 -5.2385e-03  4e-08  2e-16  8e-14\n",
      "Optimal solution found.\n",
      "0.9635\n"
     ]
    }
   ],
   "source": [
    "# 仅使用50%的数据\n",
    "print(\"Polynomial kernel\")\n",
    "model = svm.SVMClassifier()\n",
    "model.fit(train_features, train_labels, 'Polynomial', 1)\n",
    "pred_labels = model.predict(train_features)\n",
    "print(accuracy_score(train_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid kernel\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.7920e+07  6.9336e+07  1e+10  3e+02  1e-07\n",
      " 1: -1.2813e+08  9.9493e+07  1e+10  3e+02  1e-07\n",
      " 2: -1.5842e+08  1.2975e+08  1e+10  3e+02  2e-07\n",
      " 3: -1.5381e+08  1.2510e+08  1e+10  3e+02  2e-07\n",
      " 4: -2.7675e+08  2.4798e+08  1e+10  3e+02  2e-07\n",
      " 5: -2.7904e+08  2.5025e+08  1e+10  3e+02  2e-07\n",
      " 6: -2.6209e+08  2.3325e+08  1e+10  3e+02  2e-07\n",
      " 7: -2.6178e+08  2.3296e+08  1e+10  3e+02  2e-07\n",
      " 8: -3.2748e+08  2.9863e+08  1e+10  3e+02  2e-07\n",
      " 9: -3.3508e+08  3.0620e+08  1e+10  3e+02  2e-07\n",
      "10: -3.2525e+08  2.9636e+08  1e+10  3e+02  2e-07\n",
      "11: -3.2098e+08  2.9209e+08  1e+10  3e+02  2e-07\n",
      "12: -3.1778e+08  2.8888e+08  1e+10  3e+02  2e-07\n",
      "13: -3.1668e+08  2.8778e+08  1e+10  3e+02  2e-07\n",
      "14: -3.1589e+08  2.8699e+08  1e+10  3e+02  2e-07\n",
      "15: -3.1543e+08  2.8653e+08  1e+10  3e+02  2e-07\n",
      "16: -3.1506e+08  2.8615e+08  1e+10  3e+02  2e-07\n",
      "17: -3.1481e+08  2.8590e+08  1e+10  3e+02  2e-07\n",
      "18: -3.1459e+08  2.8568e+08  1e+10  3e+02  2e-07\n",
      "19: -3.1443e+08  2.8552e+08  1e+10  3e+02  2e-07\n",
      "20: -3.1428e+08  2.8538e+08  1e+10  3e+02  2e-07\n",
      "21: -3.1417e+08  2.8527e+08  1e+10  3e+02  2e-07\n",
      "22: -3.1407e+08  2.8516e+08  1e+10  3e+02  2e-07\n",
      "23: -3.1399e+08  2.8508e+08  1e+10  3e+02  2e-07\n",
      "24: -3.1391e+08  2.8500e+08  1e+10  3e+02  2e-07\n",
      "25: -3.1385e+08  2.8494e+08  1e+10  3e+02  2e-07\n",
      "26: -3.1378e+08  2.8488e+08  1e+10  3e+02  2e-07\n",
      "27: -3.1374e+08  2.8483e+08  1e+10  3e+02  2e-07\n",
      "28: -3.1369e+08  2.8478e+08  1e+10  3e+02  2e-07\n",
      "29: -3.1365e+08  2.8474e+08  1e+10  3e+02  2e-07\n",
      "30: -3.1361e+08  2.8470e+08  1e+10  3e+02  2e-07\n",
      "31: -3.1357e+08  2.8467e+08  1e+10  3e+02  2e-07\n",
      "32: -3.1354e+08  2.8463e+08  1e+10  3e+02  2e-07\n",
      "33: -3.1351e+08  2.8461e+08  1e+10  3e+02  2e-07\n",
      "34: -3.1348e+08  2.8458e+08  1e+10  3e+02  2e-07\n",
      "35: -3.1346e+08  2.8455e+08  1e+10  3e+02  2e-07\n",
      "36: -3.1343e+08  2.8453e+08  1e+10  3e+02  2e-07\n",
      "37: -3.1342e+08  2.8451e+08  1e+10  3e+02  2e-07\n",
      "38: -3.1339e+08  2.8448e+08  1e+10  3e+02  2e-07\n",
      "39: -3.1338e+08  2.8447e+08  1e+10  3e+02  2e-07\n",
      "40: -3.1336e+08  2.8445e+08  1e+10  3e+02  2e-07\n",
      "41: -3.1334e+08  2.8443e+08  1e+10  3e+02  2e-07\n",
      "42: -3.1332e+08  2.8441e+08  1e+10  3e+02  2e-07\n",
      "43: -3.1331e+08  2.8440e+08  1e+10  3e+02  2e-07\n",
      "44: -3.1329e+08  2.8439e+08  1e+10  3e+02  2e-07\n",
      "45: -3.1328e+08  2.8437e+08  1e+10  3e+02  2e-07\n",
      "46: -3.1327e+08  2.8436e+08  1e+10  3e+02  2e-07\n",
      "47: -3.1326e+08  2.8435e+08  1e+10  3e+02  2e-07\n",
      "48: -3.1324e+08  2.8434e+08  1e+10  3e+02  2e-07\n",
      "49: -3.1324e+08  2.8433e+08  1e+10  3e+02  2e-07\n",
      "50: -3.1322e+08  2.8432e+08  1e+10  3e+02  2e-07\n",
      "51: -3.1322e+08  2.8431e+08  1e+10  3e+02  2e-07\n",
      "52: -3.1320e+08  2.8430e+08  1e+10  3e+02  2e-07\n",
      "53: -3.1320e+08  2.8429e+08  1e+10  3e+02  2e-07\n",
      "54: -3.1319e+08  2.8428e+08  1e+10  3e+02  2e-07\n",
      "55: -3.1318e+08  2.8427e+08  1e+10  3e+02  2e-07\n",
      "56: -3.1317e+08  2.8426e+08  1e+10  3e+02  2e-07\n",
      "57: -3.1316e+08  2.8426e+08  1e+10  3e+02  2e-07\n",
      "58: -3.1316e+08  2.8425e+08  1e+10  3e+02  2e-07\n",
      "59: -3.1315e+08  2.8424e+08  1e+10  3e+02  2e-07\n",
      "60: -3.1314e+08  2.8423e+08  1e+10  3e+02  2e-07\n",
      "61: -3.1314e+08  2.8423e+08  1e+10  3e+02  2e-07\n",
      "62: -3.1313e+08  2.8422e+08  1e+10  3e+02  2e-07\n",
      "63: -3.1312e+08  2.8422e+08  1e+10  3e+02  2e-07\n",
      "64: -3.1312e+08  2.8421e+08  1e+10  3e+02  2e-07\n",
      "65: -3.1311e+08  2.8420e+08  1e+10  3e+02  2e-07\n",
      "66: -3.1311e+08  2.8420e+08  1e+10  3e+02  2e-07\n",
      "67: -3.1310e+08  2.8419e+08  1e+10  3e+02  2e-07\n",
      "68: -3.1310e+08  2.8419e+08  1e+10  3e+02  2e-07\n",
      "69: -3.1309e+08  2.8418e+08  1e+10  3e+02  2e-07\n",
      "70: -3.1309e+08  2.8418e+08  1e+10  3e+02  2e-07\n",
      "71: -3.1308e+08  2.8417e+08  1e+10  3e+02  2e-07\n",
      "72: -3.1308e+08  2.8417e+08  1e+10  3e+02  2e-07\n",
      "73: -3.1307e+08  2.8417e+08  1e+10  3e+02  2e-07\n",
      "74: -3.1307e+08  2.8416e+08  1e+10  3e+02  2e-07\n",
      "75: -3.1307e+08  2.8416e+08  1e+10  3e+02  2e-07\n",
      "76: -3.1306e+08  2.8415e+08  1e+10  3e+02  2e-07\n",
      "77: -3.1306e+08  2.8415e+08  1e+10  3e+02  2e-07\n",
      "78: -3.1305e+08  2.8414e+08  1e+10  3e+02  2e-07\n",
      "79: -3.1305e+08  2.8414e+08  1e+10  3e+02  2e-07\n",
      "80: -3.1305e+08  2.8414e+08  1e+10  3e+02  2e-07\n",
      "81: -3.1304e+08  2.8414e+08  1e+10  3e+02  2e-07\n",
      "82: -3.1304e+08  2.8413e+08  1e+10  3e+02  2e-07\n",
      "83: -3.1304e+08  2.8413e+08  1e+10  3e+02  2e-07\n",
      "84: -3.1303e+08  2.8412e+08  1e+10  3e+02  2e-07\n",
      "85: -3.1303e+08  2.8412e+08  1e+10  3e+02  2e-07\n",
      "86: -3.1303e+08  2.8412e+08  1e+10  3e+02  2e-07\n",
      "87: -3.1302e+08  2.8412e+08  1e+10  3e+02  2e-07\n",
      "88: -3.1302e+08  2.8411e+08  1e+10  3e+02  2e-07\n",
      "89: -3.1302e+08  2.8411e+08  1e+10  3e+02  2e-07\n",
      "90: -3.1302e+08  2.8411e+08  1e+10  3e+02  2e-07\n",
      "91: -3.1301e+08  2.8411e+08  1e+10  3e+02  2e-07\n",
      "92: -3.1301e+08  2.8410e+08  1e+10  3e+02  2e-07\n",
      "93: -3.1301e+08  2.8410e+08  1e+10  3e+02  2e-07\n",
      "94: -3.1301e+08  2.8410e+08  1e+10  3e+02  2e-07\n",
      "95: -3.1300e+08  2.8410e+08  1e+10  3e+02  2e-07\n",
      "96: -3.1300e+08  2.8409e+08  1e+10  3e+02  2e-07\n",
      "97: -3.1300e+08  2.8409e+08  1e+10  3e+02  2e-07\n",
      "98: -3.1300e+08  2.8409e+08  1e+10  3e+02  2e-07\n",
      "99: -3.1299e+08  2.8409e+08  1e+10  3e+02  2e-07\n",
      "100: -3.1299e+08  2.8408e+08  1e+10  3e+02  2e-07\n",
      "Terminated (maximum number of iterations reached).\n",
      "0.5383333333333333\n"
     ]
    }
   ],
   "source": [
    "# 仅使用30%的数据\n",
    "print(\"Sigmoid kernel\")\n",
    "model = svm.SVMClassifier()\n",
    "model.fit(train_features, train_labels, 'Sigmoid', 5)\n",
    "pred_labels = model.predict(train_features)\n",
    "print(accuracy_score(train_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the above experimental results that when Gaussians and polynomials are used as kernels, training is easier and the training results are better. When using sigmoid as the core, training is more difficult, the amount of data it can process is relatively small, the training results are not good, and the training time is also very long. This may be related to the selection of hyperparameter values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
